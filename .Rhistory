metrics_test_tree[c(1,3)]
# choose Cp with mininum xerror and prune the tree
rev_tree_prune <- prune(rev_tree, cp = rev_tree$cptable[6,"CP"])
# testing
rev_tree_pred.test <- predict(rev_tree_prune, newdata = test1_full)
actual_pred_tree.test <- as.data.frame(cbind(actuals = test1_full$revenue, predicteds = rev_tree_pred.test))
metrics_test_tree <- regr.eval(actual_pred_tree.test$actuals, actual_pred_tree.test$predicteds)
metrics_test_tree[c(1,3)]
# choose Cp with mininum xerror and prune the tree
rev_tree_prune <- prune(rev_tree, cp = rev_tree$cptable[7,"CP"])
# testing
rev_tree_pred.test <- predict(rev_tree_prune, newdata = test1_full)
actual_pred_tree.test <- as.data.frame(cbind(actuals = test1_full$revenue, predicteds = rev_tree_pred.test))
metrics_test_tree <- regr.eval(actual_pred_tree.test$actuals, actual_pred_tree.test$predicteds)
metrics_test_tree[c(1,3)]
# choose Cp with mininum xerror and prune the tree
rev_tree_prune <- prune(rev_tree, cp = rev_tree$cptable[8,"CP"])
# testing
rev_tree_pred.test <- predict(rev_tree_prune, newdata = test1_full)
actual_pred_tree.test <- as.data.frame(cbind(actuals = test1_full$revenue, predicteds = rev_tree_pred.test))
metrics_test_tree <- regr.eval(actual_pred_tree.test$actuals, actual_pred_tree.test$predicteds)
metrics_test_tree[c(1,3)]
# choose Cp with mininum xerror and prune the tree
rev_tree_prune <- prune(rev_tree, cp = rev_tree$cptable[9,"CP"])
# testing
rev_tree_pred.test <- predict(rev_tree_prune, newdata = test1_full)
actual_pred_tree.test <- as.data.frame(cbind(actuals = test1_full$revenue, predicteds = rev_tree_pred.test))
metrics_test_tree <- regr.eval(actual_pred_tree.test$actuals, actual_pred_tree.test$predicteds)
metrics_test_tree[c(1,3)]
# choose Cp with mininum xerror and prune the tree
rev_tree_prune <- prune(rev_tree, cp = rev_tree$cptable[8,"CP"])
# testing
rev_tree_pred.test <- predict(rev_tree_prune, newdata = test1_full)
actual_pred_tree.test <- as.data.frame(cbind(actuals = test1_full$revenue, predicteds = rev_tree_pred.test))
metrics_test_tree <- regr.eval(actual_pred_tree.test$actuals, actual_pred_tree.test$predicteds)
metrics_test_tree[c(1,3)]
# training
rev_tree_pred.train <- predict(rev_tree_prune)
actual_pred_tree.train <- as.data.frame(cbind(actuals = train1_full$revenue, predicteds = rev_tree_pred.train))
metrics_train_tree <- regr.eval(actual_pred_tree.train$actuals, actual_pred_tree.train$predicteds)
metrics_train_tree[c(1,3)]
# we can see the results are the same as in the original tree
# choose Cp with mininum xerror and prune the tree
rev_tree_prune <- prune(rev_tree, cp = rev_tree$cptable[10,"CP"])
# choose Cp with mininum xerror and prune the tree
rev_tree_prune <- prune(rev_tree, cp = rev_tree$cptable[8,"CP"])
# testing
rev_tree_pred.test <- predict(rev_tree_prune, newdata = test1_full)
actual_pred_tree.test <- as.data.frame(cbind(actuals = test1_full$revenue, predicteds = rev_tree_pred.test))
metrics_test_tree <- regr.eval(actual_pred_tree.test$actuals, actual_pred_tree.test$predicteds)
metrics_test_tree[c(1,3)]
# training
rev_tree_pred.train <- predict(rev_tree_prune)
actual_pred_tree.train <- as.data.frame(cbind(actuals = train1_full$revenue, predicteds = rev_tree_pred.train))
metrics_train_tree <- regr.eval(actual_pred_tree.train$actuals, actual_pred_tree.train$predicteds)
metrics_train_tree[c(1,3)]
# we can see the results are the same as in the original tree
# plot tree
prp(prf_dt, main = "Classification Trees for Profit Status")
#rpart.plot(prf_dt,main = "Classification Trees for Profit Status")
fancyRpartPlot(prf_dt, main = "Classification Trees for Profit Status")
printcp(prf_dt) # display the results
plotcp(prf_dt) # visualize cross-validation results
prf_dt_prune <- prune(prf_dt, cp = prf_dt$cptable[which.min(prf_dt$cptable[,"xerror"]),"CP"]) # prune trees at cp where xerror is minimum
# plot tree
prp(prf_dt_prune, main = "Classification Trees for Profit Status")
#rpart.plot(prf_dt_prune,main = "Classification Trees for Profit Status")
fancyRpartPlot(prf_dt_prune, main = "Classification Trees for Profit Status")
prf_dt.pred <- predict(prf_dt_prune, test3, type = "class" )
table_dt <- table(actual = test3$y, predicted = prf_dt.pred)
table_dt
# Accuracy
sum(diag(table_dt))/sum(table_dt)
#length(prf_dt.pred)
#ROC
prf_dt.pred1 <- predict(prf_dt_prune, test3, type = "prob" )
#prf_dt.pred1
prf_dt.conf <- confusionMatrix(data = as.factor(as.integer(prf_dt.pred1[,2]>0.5)), reference = test3$y)
# the accuracy will be the same as the accuracy we calculate when setting type=class, we can see the results to check it
prf_dt.conf
h_dt <- roc(test3$y ~ prf_dt.pred1[,2])
auc(h_dt)
plot(h_dt)
prf_dt.pred <- predict(prf_dt_prune, test3, type = "class" ) # predict the model on the testing data
table_dt <- table(actual = test3$y, predicted = prf_dt.pred) # create a confusion matrix of predicted and actual values
table_dt
# Accuracy Calculation
sum(diag(table_dt))/sum(table_dt) # sum(true positive + false negative)/ total
#length(prf_dt.pred)
#ROC
prf_dt.pred1 <- predict(prf_dt_prune, test3, type = "prob" )
#prf_dt.pred1
prf_dt.conf <- confusionMatrix(data = as.factor(as.integer(prf_dt.pred1[,2]>0.5)), reference = test3$y)
# the accuracy will be the same as the accuracy we calculate when setting type=class, we can see the results to check it
prf_dt.conf
h_dt <- roc(test3$y ~ prf_dt.pred1[,2])
auc(h_dt)
plot(h_dt)
# I see the method to plot ROC for classification tree in youtube and stackoverflow
# There is an option: type = "prob" when predict rpart object, it return predicted probabilities by the classification tree
# I then realize that the observations with predicted probabilites > 0.5 are the values predicted as 1 in the above method (chunk Q92)
#ROC
prf_dt.pred1 <- predict(prf_dt_prune, test3, type = "prob" )
prf_dt.pred[1:20]
as.integer(prf_dt.pred1[,2]>0.5)[1:20]
prf_dt.conf <- confusionMatrix(data = as.factor(as.integer(prf_dt.pred1[,2]>0.5)), reference = test3$y)
# the accuracy will be the same as the accuracy we calculate when setting type=class, we can see the results to check it
prf_dt.conf
h_dt <- roc(test3$y ~ prf_dt.pred1[,2])
auc(h_dt)
plot(h_dt)
# I see the method to plot ROC for classification tree in youtube and stackoverflow
# There is an option: type = "prob" when predict rpart object, it return predicted probabilities by the classification tree
# I then realize that the observations with predicted probabilites > 0.5 are the values predicted as 1 in the above method (chunk Q92)
#ROC
prf_dt.pred1 <- predict(prf_dt_prune, test3, type = "prob" )
prf_dt.pred[1:20]
as.factor(as.integer(prf_dt.pred1[,2]>0.5))[1:20]
prf_dt.conf <- confusionMatrix(data = as.factor(as.integer(prf_dt.pred1[,2]>0.5)), reference = test3$y)
# the accuracy will be the same as the accuracy we calculate when setting type=class, we can see the results to check it
prf_dt.conf
h_dt <- roc(test3$y ~ prf_dt.pred1[,2])
auc(h_dt)
plot(h_dt)
# I see the method to plot ROC for classification tree in youtube and stackoverflow
# There is an option: type = "prob" when predict rpart object, it return predicted probabilities by the classification tree
# I then realize that the observations with predicted probabilites > 0.5 are the values predicted as 1 in the above method (chunk Q92)
#ROC
prf_dt.pred1 <- predict(prf_dt_prune, test3, type = "prob" )
prf_dt.pred[1:20]
prf_dt.pred1[1:20]
prf_dt.conf <- confusionMatrix(data = as.factor(as.integer(prf_dt.pred1[,2]>0.5)), reference = test3$y)
# the accuracy will be the same as the accuracy we calculate when setting type=class, we can see the results to check it
prf_dt.conf
h_dt <- roc(test3$y ~ prf_dt.pred1[,2])
auc(h_dt)
plot(h_dt)
prf_dt.pred
# I see the method to plot ROC for classification tree in youtube and stackoverflow
# There is an option: type = "prob" when predict rpart object, it return predicted probabilities by the classification tree
# I then realize that the observations with predicted probabilites > 0.5 are the values predicted as 1 in the above method (chunk Q92)
#ROC
prf_dt.pred1 <- predict(prf_dt_prune, test3, type = "prob" )
prf_dt.pred[406]
prf_dt.pred1[406]
prf_dt.conf <- confusionMatrix(data = as.factor(as.integer(prf_dt.pred1[,2]>0.5)), reference = test3$y)
# the accuracy will be the same as the accuracy we calculate when setting type=class, we can see the results to check it
prf_dt.conf
h_dt <- roc(test3$y ~ prf_dt.pred1[,2])
auc(h_dt)
plot(h_dt)
prf_dt.pred <- predict(prf_dt_prune, test3, type = "class" ) # predict the model on the testing data
table_dt <- table(actual = test3$y, predicted = prf_dt.pred) # create a confusion matrix of predicted and actual values
table_dt
# Accuracy Calculation
sum(diag(table_dt))/sum(table_dt) # sum(true positive + false negative)/ total
#length(prf_dt.pred)
# I see the method to plot ROC for classification tree in youtube and stackoverflow
# There is an option: type = "prob" when predict rpart object, it return predicted probabilities by the classification tree
# I then realize that the observations with predicted probabilites > 0.5 are the values predicted as 1 in the above method (chunk Q92)
#ROC
prf_dt.pred1 <- predict(prf_dt_prune, test3, type = "prob" )
prf_dt.pred[2978]
prf_dt.pred1[406]
prf_dt.conf <- confusionMatrix(data = as.factor(as.integer(prf_dt.pred1[,2]>0.5)), reference = test3$y)
# the accuracy will be the same as the accuracy we calculate when setting type=class, we can see the results to check it
prf_dt.conf
h_dt <- roc(test3$y ~ prf_dt.pred1[,2])
auc(h_dt)
plot(h_dt)
# I see the method to plot ROC for classification tree in youtube and stackoverflow
# There is an option: type = "prob" when predict rpart object, it return predicted probabilities by the classification tree
# I then realize that the observations with predicted probabilites > 0.5 are the values predicted as 1 in the above method (chunk Q92)
#ROC
prf_dt.pred1 <- predict(prf_dt_prune, test3, type = "prob" )
lenght(prf_dt.pred)
# I see the method to plot ROC for classification tree in youtube and stackoverflow
# There is an option: type = "prob" when predict rpart object, it return predicted probabilities by the classification tree
# I then realize that the observations with predicted probabilites > 0.5 are the values predicted as 1 in the above method (chunk Q92)
#ROC
prf_dt.pred1 <- predict(prf_dt_prune, test3, type = "prob" )
lenght(prf_dt.pred)
# I see the method to plot ROC for classification tree in youtube and stackoverflow
# There is an option: type = "prob" when predict rpart object, it return predicted probabilities by the classification tree
# I then realize that the observations with predicted probabilites > 0.5 are the values predicted as 1 in the above method (chunk Q92)
#ROC
prf_dt.pred1 <- predict(prf_dt_prune, test3, type = "prob" )
length(prf_dt.pred)
nrow(prf_dt.pred1)
prf_dt.conf <- confusionMatrix(data = as.factor(as.integer(prf_dt.pred1[,2]>0.5)), reference = test3$y)
# the accuracy will be the same as the accuracy we calculate when setting type=class, we can see the results to check it
prf_dt.conf
h_dt <- roc(test3$y ~ prf_dt.pred1[,2])
auc(h_dt)
plot(h_dt)
# I see the method to plot ROC for classification tree in youtube and stackoverflow
# There is an option: type = "prob" when predict rpart object, it return predicted probabilities by the classification tree
# I then realize that the values with predicted probabilites > 0.5 are the values predicted as 1 in the above method (chunk Q92)
# I create the another confusion matrix for this part and set threshold = 0.5 and see that the confusion matrices are also the same
# prediction using type = "prob"
prf_dt.pred1 <- predict(prf_dt_prune, test3, type = "prob" )
prf_dt.conf <- confusionMatrix(data = as.factor(as.integer(prf_dt.pred1[,2]>0.5)), reference = test3$y)
# the accuracy will be the same as the accuracy we calculate when setting type=class, we can see the results to check it
prf_dt.conf
#plot ROC and see AUC
h_dt <- roc(test3$y ~ prf_dt.pred1[,2])
auc(h_dt)
plot(h_dt)
loadPkg("class")
# we use the same train and test sets as in the linear model section
#train2 <- as.data.frame(scale(movie_num, center = TRUE, scale = TRUE))
#train2$genres <- movie[movie_sample==1, 1:14]$genres
#train set
train2 <- train1_full  # dupicate the train set with all predictors we created above
train2$revenue <- scale(train2$revenue, center = TRUE, scale = TRUE) # scale the revenue column, other numerical variables were scaled before
# numerical colums range from 1:6; 7:9 are our reponses
# same with test set
test2 <- test1_full
test2$revenue <- scale(test2$revenue, center = TRUE, scale = TRUE)
#str(train2)
#str(test2)
loadPkg("class")
# we use the same train and test sets as in the linear model section
#train2 <- as.data.frame(scale(movie_num, center = TRUE, scale = TRUE))
#train2$genres <- movie[movie_sample==1, 1:14]$genres
#train set
train2 <- train1_full  # dupicate the train set with all predictors we created above
train2$revenue <- scale(train2$revenue, center = TRUE, scale = TRUE) # scale the revenue column, other numerical variables were scaled before
# numerical colums range from 1:6; 7:9 are our reponses
# same with test set
test2 <- test1_full
test2$revenue <- scale(test2$revenue, center = TRUE, scale = TRUE)
#str(train2)
#str(test2)
season_knn <- knn(train = train2[,c(1:6)], test = test2[,c(1:6)], cl=train2$season, k=3)
loadPkg("gmodels")
crosstab <- CrossTable(test2$season, season_knn, prop.chisq = FALSE)
chooseK = function(k, train_set, val_set, train_class, val_class){
# Build knn with k neighbors considered.
class_knn = knn(train = train_set,    #<- training set cases
test = val_set,       #<- test set cases
cl = train_class,     #<- category for classification
k = k,                #<- number of neighbors considered
use.all = TRUE)       #<- control ties between class assignments
#   If true, all distances equal to the kth
#   largest are included
tab = table(class_knn, val_class)
# Calculate the accuracy.
accu = sum(tab[row(tab) == col(tab)]) / sum(tab)
cbind(k = k, accuracy = accu)
}
knn_season_k = sapply(seq(1, 41, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = train2[c(1:6)],
val_set = test2[c(1:6)],
train_class = train2$season,
val_class = test2$season))
knn_season_k = data.frame(k = knn_season_k[1,],
accuracy = knn_season_k[2,])
# Plot accuracy vs. k.
ggplot(knn_season_k,
aes(x = k, y = accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3)
knn_genres_k = sapply(seq(1, 41, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = train2[c(1:6)],
val_set = test2[c(1:6)],
train_class = train2$genres,
val_class = test2$genres))
knn_genres_k = data.frame(k = knn_genres_k[1,],
accuracy = knn_genres_k[2,])
# Plot accuracy vs. k.
ggplot(knn_genres_k,
aes(x = k, y = accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3)
set.seed(123)
season_knn <- knn(train = train2[,c(1:6)], test = test2[,c(1:6)], cl=train2$season, k=9)
loadPkg("gmodels")
crosstab <- CrossTable(test2$season, season_knn, prop.chisq = FALSE)
set.seed(123)
season_knn <- knn(train = train2[,c(1:6)], test = test2[,c(1:6)], cl=train2$season, k=9)
loadPkg("gmodels")
crosstab <- CrossTable(test2$season, season_knn, prop.chisq = FALSE)
set.seed(123)
knn_genres_k = sapply(seq(1, 41, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = train2[c(1:6)],
val_set = test2[c(1:6)],
train_class = train2$genres,
val_class = test2$genres))
knn_genres_k = data.frame(k = knn_genres_k[1,],
accuracy = knn_genres_k[2,])
# Plot accuracy vs. k.
ggplot(knn_genres_k,
aes(x = k, y = accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3)
set.seed(123)
knn_season_k = sapply(seq(1, 41, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = train2[c(1:6)],
val_set = test2[c(1:6)],
train_class = train2$season,
val_class = test2$season))
knn_season_k = data.frame(k = knn_season_k[1,],
accuracy = knn_season_k[2,])
# Plot accuracy vs. k.
ggplot(knn_season_k,
aes(x = k, y = accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3)
set.seed(123)
company_knn <- knn(train = train2[,c(1:6)], test = test2[,c(1:6)], cl=train2$company, k=3)
crosstab <- CrossTable(test2$company, company_knn, prop.chisq = FALSE)
set.seed(123)
company_knn <- knn(train = train2[,c(1:6)], test = test2[,c(1:6)], cl=train2$company, k=3)
crosstab <- CrossTable(actuals = test2$company, predicted = company_knn, prop.chisq = FALSE)
set.seed(123)
company_knn <- knn(train = train2[,c(1:6)], test = test2[,c(1:6)], cl=train2$company, k=3)
crosstab <- CrossTable(test2$company, company_knn, prop.chisq = FALSE)
set.seed(123)
knn_company_k = sapply(seq(1, 41, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = train2[c(1:6)],
val_set = test2[c(1:6)],
train_class = train2$company,
val_class = test2$company))
knn_company_k = data.frame(k = knn_company_k[1,],
accuracy = knn_company_k[2,])
# Plot accuracy vs. k.
ggplot(knn_company_k,
aes(x = k, y = accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3)
set.seed(123)
company_knn <- knn(train = train2[,c(1:6)], test = test2[,c(1:6)], cl=train2$company, k=9)
crosstab <- CrossTable(test2$company, company_knn, prop.chisq = FALSE)
set.seed(123)
knn_company_k = sapply(seq(1, 41, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = train2[c(1:6)],
val_set = test2[c(1:6)],
train_class = train2$company,
val_class = test2$company))
knn_company_k = data.frame(k = knn_company_k[1,],
accuracy = knn_company_k[2,])
# Plot accuracy vs. k.
ggplot(knn_company_k,
aes(x = k, y = accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3)
#loadPkg("")
movie_ts <- subset(movie, (year > 1994))
movie_ts <- subset(movie_ts, select = c("revenue", "year", "quarter"))
movie_ts <- movie_ts[order(movie_ts$year),]
#unique(movie_ts$year)
str(movie_ts)
loadPkg("dplyr")
movie_ts = movie_ts %>% group_by(year, quarter) %>% summarise_each(funs(sum))
movie_ts$quarter <- factor(movie_ts$quarter, levels = c("Q1", "Q2", "Q3", "Q4"))
movie_ts <- movie_ts[order(movie_ts$year, movie_ts$quarter),]
movie_ts
#loadPkg("")
movie_ts <- subset(movie, (year > 1994))
movie_ts <- subset(movie_ts, select = c("revenue", "year", "quarter"))
movie_ts <- movie_ts[order(movie_ts$year),]
#unique(movie_ts$year)
#str(movie_ts)
loadPkg("dplyr")
movie_ts = movie_ts %>% group_by(year, quarter) %>% summarise_each(funs(sum))
movie_ts$quarter <- factor(movie_ts$quarter, levels = c("Q1", "Q2", "Q3", "Q4"))
movie_ts <- movie_ts[order(movie_ts$year, movie_ts$quarter),]
#movie_ts
plot(movie.ts)
plot(movie.ts)
plot(movie.ts, xlab="revenue")
plot(movie.ts, xlab = "year", ylab="revenue")
movie_dcp <- decompose(movie.ts)
summary(movie_dcp$seasonal)
summary(movie_dcp$random)
plot(movie_dcp)
movie.ts.smth <- HoltWinters(movie.ts)
movie.ts.smth
movie.ts.smth$SSE
plot(movie.ts.smth)
plot(movie.ts, xlab = "Year", ylab="Revenue")
plot(movie.ts.smth)
loadPkg("forecast")
#accuracy(movie_hw)
movie_ft <- forecast(movie.ts.smth, h=23)
plot(movie_ft)
#sm <- ma(movie.ts, order=4) # 4 quarters moving average
#lines(sm, col="red") # plot
movie_arima <- auto.arima(movie.ts)
movie_arima <- forecast(movie_arima, h=23)
plot(movie_arima)
#movie_arima
movie_arima <- auto.arima(movie.ts)
movie_arima <- forecast(movie_arima, h=23)
plot(movie_arima)
#movie_arima
loadPkg("forecast")
#accuracy(movie_hw)
movie_ft <- forecast(movie.ts.smth, h=23)
plot(movie_ft)
#sm <- ma(movie.ts, order=4) # 4 quarters moving average
#lines(sm, col="red") # plot
loadPkg("highcharter")
highchart(type="chart") %>%        # try type = "chart", "stock" for different visualizations
hc_chart(type = "column") %>%      # try type = "line", bar", "column" for different visualizations
hc_add_series_ts(movie.ts.test,name = "ACTUAL", color = "red" ) %>%   # plot the actual data labeled by red color
hc_add_series(name = "PREDICTED", movie_ft, color = "blue" )  %>%     # plot the hw predictions labeled by blue color
hc_legend(align = "left", verticalAlign = "top",
layout = "vertical", x = 0, y = 100)
hchart(movie_ft) %>%
hc_add_series_ts(movie.ts.test, color = "red", name = "ACTUAL")
plot(movie_ft)
lines(movie.ts.test, col = 'red')
#ggplot(movie_hw)
legend("topleft",lty=1,bty = "n",col=c("red","blue"),c("actual","predicted"))
plot(movie_ft)
lines(movie.ts.test, col = 'red')
#ggplot(movie_hw)
legend("topleft",lty=1,bty = "n",col=c("red","blue"),c("actual","predicted"))
loadPkg("highcharter")
highchart(type="chart") %>%        # try type = "chart", "stock" for different visualizations
hc_chart(type = "column") %>%      # try type = "line", bar", "column" for different visualizations
hc_add_series_ts(movie.ts.test,name = "ACTUAL", color = "red" ) %>%   # plot the actual data labeled by red color
hc_add_series(name = "PREDICTED", movie_ft, color = "blue" )  %>%     # plot the hw predictions labeled by blue color
hc_legend(align = "left", verticalAlign = "top",
layout = "vertical", x = 0, y = 100)
hchart(movie_ft) %>%
hc_add_series_ts(movie.ts.test, color = "red", name = "ACTUAL")
movie_arima <- auto.arima(movie.ts) # create arima model
plot(movie_arima)
movie_arima <- forecast(movie_arima, h=23) # see how it forcast
plot(movie_arima)
#movie_arima
movie_arima <- auto.arima(movie.ts) # create arima model
movie_arima <- forecast(movie_arima, h=23) # see how it forcast
plot(movie_arima)
#movie_arima
highchart(type="stock") %>%        # try type = "chart", "stock" for different visualizations
hc_chart(type = "line") %>%      # try type = "line", bar", "column" for different visualizations
hc_add_series_ts(movie.ts.test,name = "ACTUAL", color = "red" ) %>%   # plot the actual data labeled by red color
hc_add_series(name = "PREDICTED", movie_arima, color = "blue" )  %>%     # plot the hw predictions labeled by blue color
hc_legend(align = "left", verticalAlign = "top",
layout = "vertical", x = 0, y = 100)
hchart(movie_arima) %>%
hc_add_series_ts(movie.ts.test, color = "red", name = "ACTUAL")
plot(movie_arima)
lines(movie.ts.test, col = 'red')
legend("topleft",lty=1,bty = "n",col=c("red","blue"),c("actual","predicted"))
highchart(type="stock") %>%        # try type = "chart", "stock" for different visualizations
hc_chart(type = "line") %>%      # try type = "line", bar", "column" for different visualizations
hc_add_series_ts(movie.ts.test,name = "ACTUAL", color = "red" ) %>%   # plot the actual data labeled by red color
hc_add_series(name = "PREDICTED", movie_arima, color = "blue" )  %>%     # plot the hw predictions labeled by blue color
hc_legend(align = "left", verticalAlign = "top",
layout = "vertical", x = 0, y = 100)
hchart(movie_arima) %>%
hc_add_series_ts(movie.ts.test, color = "red", name = "ACTUAL")
loadPkg("highcharter")
hchart(movie_ft) %>%
hc_add_series_ts(movie.ts.test, color = "red", name = "ACTUAL") # plot the predicted and actual values in testing data, and the values in training data
# we can have a more detailed plot with only predicted and actual values in testing data
highchart(type="chart") %>%        # try type = "chart", "stock" for different visualizations
hc_chart(type = "column") %>%      # try type = "line", bar", "column" for different visualizations
hc_add_series_ts(movie.ts.test,name = "ACTUAL", color = "red" ) %>%   # plot the actual data labeled by red color
hc_add_series(name = "PREDICTED", movie_ft, color = "blue" )  %>%     # plot the hw predictions labeled by blue color
hc_legend(align = "left", verticalAlign = "top",
layout = "vertical", x = 0, y = 100)
movie_arima <- auto.arima(movie.ts) # create arima model
movie_arima <- forecast(movie_arima, h=23) # see how it forcast
plot(movie_arima)
#movie_arima
plot(movie_arima)
lines(movie.ts.test, col = 'red')
legend("topleft",lty=1,bty = "n",col=c("red","blue"),c("actual","predicted"))
hchart(movie_arima) %>%
hc_add_series_ts(movie.ts.test, color = "red", name = "ACTUAL")  # general graph
# more detailed graph, Itry different stuffs here too
highchart(type="stock") %>%        # try type = "chart", "stock" for different visualizations
hc_chart(type = "line") %>%      # try type = "line", bar", "column" for different visualizations
hc_add_series_ts(movie.ts.test,name = "ACTUAL", color = "red" ) %>%   # plot the actual data labeled by red color
hc_add_series(name = "PREDICTED", movie_arima, color = "blue" )  %>%     # plot the hw predictions labeled by blue color
hc_legend(align = "left", verticalAlign = "top",
layout = "vertical", x = 0, y = 100)
set.seed(123)
rev_tree <- rpart(revenue ~ ., method ="anova", data=train1_full) #use rpart with method = "anova"
#plotcp(rev_tree ) # visualize cross-validation results
summary(rev_tree ) # detailed summary of splits
#printcp(rev_tree ) # display the results
rev_tree.rsq = 1 - printcp(rev_tree)[9,3] # we can calculate R-quared using equation: R-squared = 1 - rel error
# I see this equation in stackoverflow.com
# I'm not sure if it is 100% correct
# visualize the tree using 2 methods prp() and fancyRpartPlot()
prp(rev_tree, main = "Classification Trees for Revenue")
fancyRpartPlot(rev_tree, main = "Classification Trees for Revenue")
rev_tree_pred.test <- predict(rev_tree, newdata = test1_full)
actual_pred_tree.test <- as.data.frame(cbind(actuals = test1_full$revenue, predicteds = rev_tree_pred.test))
metrics_test_tree <- regr.eval(actual_pred_tree.test$actuals, actual_pred_tree.test$predicteds)
metrics_test_tree[c(1,3)]
rev_tree_pred.train <- predict(rev_tree, train1_full)
actual_pred_tree.train <- as.data.frame(cbind(actuals = train1_full$revenue, predicteds = rev_tree_pred.train))
metrics_train_tree <- regr.eval(actual_pred_tree.train$actuals, actual_pred_tree.train$predicteds)
metrics_train_tree[c(1,3)]
rev_tree <- rpart(revenue ~ ., method ="anova", data=train1_full) #use rpart with method = "anova"
#plotcp(rev_tree ) # visualize cross-validation results
summary(rev_tree ) # detailed summary of splits
#printcp(rev_tree ) # display the results
