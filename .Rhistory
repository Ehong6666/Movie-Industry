reg.lm1 <- regsubsets(revenue~., data = train1_full, nbest = 1, method = "exhaustive")  # leaps, exhaustive method
plot(reg.lm1, scale = "adjr2", main = "Adjusted R^2")
plot(reg.lm1, scale = "bic", main = "BIC")
plot(reg.lm1, scale = "Cp", main = "Cp")
rev_lm3 <- lm(revenue ~ budget + vote + company + genres + popularity , data = train1_full)
summary(rev_lm3)
vif(rev_lm3)
#loadPkg("MLmetrics")
rev_lm_test3 <- predict(object = rev_lm3, newdata = test1_full) # prediction on test set
rev_lm_predtest3 <- data.frame(cbind(actuals=test1_full$revenue, predicteds=rev_lm_test3))
rev_lm_metrics_test3 <- regr.eval(rev_lm_predtest3$actuals, rev_lm_predtest3$predicteds) # metrics (RMSE, MAE, MSE, MAPE)
rev_lm_metrics_test3[c(1,3)] # show the metrics
rev_lm_train3 <- predict(object = rev_lm3) # predicted values in train set
rev_lm_predtrain3 <- data.frame(cbind(actuals=train1_full$revenue, predicteds=rev_lm_train3))
rev_lm_metrics_train3 <- regr.eval(rev_lm_predtrain3$actuals, rev_lm_predtrain3$predicteds) # metrics (RMSE, MAE, MSE, MAPE)
options(scientific=T, digits = 2) # to make the format of metrics the same as above metrics (as Xe+07)
rev_lm_metrics_train3[c(1,3)]
options(scientific=T, digits = 3) # change back to our default format
AIC(object=rev_lm2) # best model with budget + popularity + vote
BIC(object=rev_lm2)
cat("\n")
AIC(object=rev_lm3) # best model including genres and company
BIC(object=rev_lm3)
rev_tree0 <- tree(revenue ~ ., data=train1_full) # use tree function
summary(rev_tree0)
plot(rev_tree0)
text(rev_tree0,cex=0.7)
rev_tree <- rpart(revenue ~ ., method ="anova", data=train1_full) #use rpart with method = "anova"
#plotcp(rev_tree ) # visualize cross-validation results
summary(rev_tree ) # detailed summary of splits
#printcp(rev_tree ) # display the results
rev_tree.rsq = 1 - printcp(rev_tree)[9,3] # we can calculate R-quared using equation: R-squared = 1 - rel error
# I see this equation in stackoverflow.com
# I'm not sure if it is 100% correct
# visualize the tree using 2 methods prp() and fancyRpartPlot()
prp(rev_tree, main = "Classification Trees for Revenue")
fancyRpartPlot(rev_tree)
rev_tree_pred.test <- predict(rev_tree, newdata = test1_full)
actual_pred_tree.test <- as.data.frame(cbind(actuals = test1_full$revenue, predicteds = rev_tree_pred.test))
metrics_test_tree <- regr.eval(actual_pred_tree.test$actuals, actual_pred_tree.test$predicteds)
metrics_test_tree[c(1,3)]
rev_tree_pred.train <- predict(rev_tree, train1_full)
actual_pred_tree.train <- as.data.frame(cbind(actuals = train1_full$revenue, predicteds = rev_tree_pred.train))
metrics_train_tree <- regr.eval(actual_pred_tree.train$actuals, actual_pred_tree.train$predicteds)
metrics_train_tree[c(1,3)]
printcp(rev_tree)
plotcp(rev_tree) # visualize cross-validation results
# prune the tree for optimization and avoid overfitting
rev_tree$cptable[,"xerror"] # we can see the lowest xerror
# choose Cp with mininum xerror and prune the tree
rev_tree_prune <- prune(rev_tree, cp = rev_tree$cptable[8,"CP"])
fancyRpartPlot(rev_tree_prune)
# choose Cp with mininum xerror and prune the tree
#rev_tree_prune <- prune(rev_tree, cp = rev_tree$cptable[8,"CP"])
# testing
rev_tree_pred.test <- predict(rev_tree_prune, newdata = test1_full)
actual_pred_tree.test <- as.data.frame(cbind(actuals = test1_full$revenue, predicteds = rev_tree_pred.test))
metrics_test_tree <- regr.eval(actual_pred_tree.test$actuals, actual_pred_tree.test$predicteds)
metrics_test_tree[c(1,3)]
# training
rev_tree_pred.train <- predict(rev_tree_prune)
actual_pred_tree.train <- as.data.frame(cbind(actuals = train1_full$revenue, predicteds = rev_tree_pred.train))
metrics_train_tree <- regr.eval(actual_pred_tree.train$actuals, actual_pred_tree.train$predicteds)
metrics_train_tree[c(1,3)]
# we can see the results are the same as in the original tree
set.seed(123) # set.seed to make sure the selection of tree samples are the same if we re-run the code
rev_rf = randomForest(revenue~. , train1_full, ntree = 350) # construct RF model on training data, we set the number of trees as 350
rev_rf # the results of the model
randomForest::importance(rev_rf) # the importance of each predictor
plot(rev_rf)
#rev_rf$mse
# predicting the model on the testing data
rev_rf.test <- predict(rev_rf, test1_full)
actual_pred_rf.test <- as.data.frame(cbind(actuals = test1_full$revenue, predicteds = rev_rf.test))
metrics_test_rf  <- regr.eval(actual_pred_rf.test$actuals, actual_pred_rf.test$predicteds)
options(scientific=T, digits = 2) # to make the format of metrics the same as above metrics (as Xe+07)
metrics_test_rf[c(1,3)]
options(scientific=T, digits = 3) # change back to our default format
#rev_rf$predicted
rev_rf.train <- predict(rev_rf) # same results as rev_rf$predicted, both codes return the predicted values in the training set
actual_pred_rf.train <- as.data.frame(cbind(actuals = train1_full$revenue, predicteds = rev_rf.train))
metrics_train_rf <- regr.eval(actual_pred_rf.train$actuals, actual_pred_rf.train$predicteds)
options(scientific=T, digits = 2) # to make the format of metrics the same as above metrics (as Xe+07)
metrics_train_rf[c(1,3)]
options(scientific=T, digits = 3) # change back to our default format
# there is a parameter called mtry which is used to tune a random forest model
# we use tuneRF() function to train different models with different mtry and see which mtry gives lowest OOB (Out-Of-Bag) Error
set.seed(123)
tune.rf <- tuneRF(x = train1_full[-c(1)], y = train1_full$revenue, ntreeTry = 350)
best_mtry <- tune.rf[,"mtry"][which.min(tune.rf[,"OOBError"])] # retrieve the mtry with lowest OOB error
#best_mtry # show the best mtry value
set.seed(123) # set.seed to make sure the selection of tree samples are the same if we re-run the code
rev_rf.tune = randomForest(revenue~. , train1_full, mtry = 4, ntree = 350)
rev_rf.tune
# predicting the model on the testing data
rev_rf.test1 <- predict(rev_rf.tune, test1_full)
actual_pred_rf.test1 <- as.data.frame(cbind(actuals = test1_full$revenue, predicteds = rev_rf.test1))
metrics_test_rf1  <- regr.eval(actual_pred_rf.test1$actuals, actual_pred_rf.test1$predicteds)
options(scientific=T, digits = 2) # change format for easier comparison with metrics from previous model
metrics_test_rf1[c(1,3)]
#rev_rf.tune$predicted
rev_rf.train1 <- predict(rev_rf.tune) # same results as rev_rf.tune$predicted, both codes return the predicted values in the training set
actual_pred_rf.train1 <- as.data.frame(cbind(actuals = train1_full$revenue, predicteds = rev_rf.train1))
metrics_train_rf1 <- regr.eval(actual_pred_rf.train1$actuals, actual_pred_rf.train1$predicteds)
metrics_train_rf1[c(1,3)]
options(scientific=T, digits = 3) # change back to our initial format
PCAxform <- function(df, z=TRUE) {
#' Obtain the dataframe with the Principal Components after the rotation.
#' ELo 201903 GWU DATS
#' @param df The dataframe.
#' @param z T/F or 0/1 for z-score to be used
#' @return The transformed dataframe.
#' @examples
#' tmp = PCAxform(USArrests,TRUE)
z = ifelse(z==TRUE || z=="true" || z=="True" || z=="T" || z=="t" || z==1 || z=="1", TRUE, FALSE) # standardize z
if(z) { df = data.frame(scale(df))}  # scale not safe for non-numeric colunms, but PCA requires all variables numerics to begin with.
nmax = length(df)
pr.out = prcomp(df,scale=z)
df1 = data.frame()
cnames = c()
for( i in 1:nmax ) {
vec = 0
cnames = c( cnames, paste("PC",i, sep="") )
for( j in 1:nmax ) { vec = vec + pr.out$rotation[j,i]*df[,j] }
if( length(df1)>0 ) { df1 = data.frame(df1,vec) } else { df1 = data.frame(vec) }
}
colnames(df1) <- cnames
return(df1)
}
# To-be-implemented: for z=TRUE, it will be better to have the z-scaling option for x-vars and y separately. It is actually convenient keep y in original units.
PCRxform <- function(df, y, zX=TRUE, zy=FALSE) {
#' Obtain the dataframe with the Principal Components after the rotation for PCRegression. Requires related function PCAxform()
#' ELo 201903 GWU DATS
#' @param df The dataframe.
#' @param y The y-variable column index number(int), or the name of y-variable
#' @param zX T/F or 0/1 for z-score used on X-variables
#' @param zy T/F or 0/1 for z-score used on the target y-variable
#' @return The transformed dataframe.
#' @examples
# take care of y target
zy = ifelse(zy==TRUE || zy=="true" || zy=="True" || zy=="T" || zy=="t" || zy==1 || zy=="1", TRUE, FALSE) # standardize target y
if( is.integer(y) ) { # y is integer
if( y>length(df) || y<1 ) {
print("Invalid column number")
return(NULL)
}
if(zy) { df1 = data.frame( scale(df[y]) ) } else { df1 = df[y] } # save y-var in df1
df = df[-y] # remove y-variable in df
} else { # y is not integer, so interpret as name
if(zy) { df1 = data.frame( scale( df[names(df) == y] ) ) } else { df1 = df[names(df) == y] }
df = df[names(df) != y] # remove y-variable in df
}
if( length(df1)<1 ) {
print("Variable name not found in data.frame")
return(NULL)
}
# now transform X-vars
zX = ifelse(zX==TRUE || zX=="true" || zX=="True" || zX=="T" || zX=="t" || zX==1 || zX=="1", TRUE, FALSE) # standardize X-vars
df2 = PCAxform(df,zX)
df1 = data.frame(df1,df2) # piece them back together
return(df1)
}
loadPkg("pls")
loadPkg("mice")
#loadPkg("ISLR")
pr = prcomp(movie_num[c(1:6)] , scale =FALSE)
pr_scale = prcomp(movie_num[c(1:6)], scale =TRUE)
summary(pr)
#pr$rotation
summary(pr_scale)
#pr_scale$rotation
pr.var <- (pr$sdev^2)
pve <- pr.var/sum(pr.var)
plot(cumsum(pve), xlab="Principal Component (standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b", main ="Non-centered version")
pr_scale.var <- (pr_scale$sdev^2)
pve_scale  <- pr_scale.var/sum(pr_scale.var)
plot(cumsum(pve_scale), xlab="Principal Component (non-standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b", main ="Centered version")
# we used the train and test sets in linear regression model
# at first we use the PCRxform function (Prof. EL) to scale and rotate
# training set
# we do not scale the response revenue but scale all predictors
movie_pcr = PCRxform(train1[c(1:6)],"revenue",TRUE,FALSE)  # scaled predictors
#movie_pcr
# testing set
movie_pcr_test = PCRxform(test1[c(1:6)],"revenue",TRUE,FALSE)
#movie_pcr_test
# non-centered version, nothing is scaled
movie_pcr.nc = PCRxform(train1[c(1:6)],"revenue",FALSE, FALSE)  # non-scaled predictors
#movie_pcr.nc
movie_pcr.nc_test = PCRxform(test1[c(1:6)],"revenue",FALSE, FALSE)
#movie_pcr.nc_test
# scaled data
pcr.fit <- pcr(revenue~.,data=movie_pcr,scale=FALSE,validation="CV") # build the model, as we scaled data before we use scale = F here
# non-scaled data
pcr.fit.nc <- pcr(revenue~.,data=movie_pcr.nc,scale=FALSE,validation="CV")
validationplot(pcr.fit.nc,val.type="MSEP", legend="topright")
validationplot(pcr.fit.nc,val.type="R2")
validationplot(pcr.fit,val.type="MSEP", legend="topright") # validation with MSEP & R2
validationplot(pcr.fit,val.type="R2")
coefplot(pcr.fit, intercept=TRUE) # plot coefficients of different components
# position 1 is the intercept, the first PC starts at position 2
# we can see the summary of the model
summary(pcr.fit.nc)
summary(pcr.fit)
variance.pcr = c(1:5) # create a vector to store the variances for different numbers of components
for (i in 1:5) {
pcr_pred <- predict(pcr.fit, movie_pcr_test , ncomp = i); # predicting the PCR model on the testing set
# calculate the variance by returning the mean of  [predicted value - mean(actual values)] ^2
variance.pcr[i] = mean((pcr_pred - mean(movie_pcr_test$revenue))^2)
# calculate the variance by taking mean of( (predicted value - mean(actual values) ^2 )
}
var.pcr.df = as.data.frame(cbind(variance=variance.pcr, components = c(1:5)))
#pred_act <- data.frame(cbind(pcr_pred, movie_pcr_test$revenue))
ggplot(data=var.pcr.df) +
geom_line(aes(x = components, y = variance)) +
geom_point(aes(x = components, y = variance)) +
theme_light()
pcr_lm5 <- lm(revenue ~ PC1 + PC2 , data = movie_pcr.nc) # linear model with PC1 and PC2
summary(pcr_lm5)
vif(pcr_lm5)
pcr_lm4 <- lm(revenue ~ PC1 + PC2 , data = movie_pcr)
summary(pcr_lm4)
vif(pcr_lm4)
AIC(object = pcr_lm4)
BIC(object = pcr_lm4)
AIC(object = pcr_lm5)
BIC(object = pcr_lm5)
#str(movie)
if (require("pls")) {detach("package:pls", unload = T)  # for some reason package "pls" made the corrplot not proper so I detach it after using
}
prd_pcr <- predict(pcr_lm4, newdata=movie_pcr_test)
regr.eval(movie_pcr_test$revenue, prd_pcr)
# Prior check the dependence of genres and profitable
contable1 <- table(movie$genres, movie$profitable)
chisqres1 = chisq.test(contable1)
chisqres1
# Prior check the dependence of company and profitable
contable2 <- table(movie$company, movie$profitable)
chisqres2 = chisq.test(contable2)
chisqres2
# Prior check the dependence of season and profitable
contable3 <- table(movie$season, movie$profitable)
chisqres3 = chisq.test(contable3)
chisqres3
# we do some preparation with the data for our logit model
movie_nd <- movie_num # duplicate the dataframe of numerical variables
# append back the other columns
movie_nd$genres = movie$genres
movie_nd$company = movie$company
movie_nd$season = movie$season
movie_nd$y = movie$profitable # profitable column (negative profit = 0, positive profit = 1) is saved as y
str(movie_nd)
# split the train and test sets with ratio 50:50
#set.seed(1)
#movie_sample1 <- sample(2, nrow(movie_scale), replace=TRUE, prob=c(0.50, 0.50))
# create train and test sets
#train2 <- movie_scale[movie_sample1==1, 1:9]
#test2 <- movie_scale[movie_sample1==2, 1:9]
loadPkg("bestglm")
res.bestglm0 <- bestglm(Xy = movie_nd, family = binomial,
IC = "AIC",                 # Information criteria for
method = "exhaustive")
#summary(res.bestglm)
res.bestglm0$BestModels
# we will use the same train and test sets as in previous chapters
# remove revenue column and use other predictors in train and test sets
train3 <- movie_nd[movie_sample == 1, 2:10] # revenue column is 1, we do not include it
test3 <- movie_nd[movie_sample == 2, 2:10]
dim(train3)
dim(test3)
prf_glm <- glm(y ~ ., data = train3, family = "binomial")
summary(prf_glm)
exp(coef(prf_glm))[24:28] # company
exp(coef(prf_glm))[7:23] # genres
exp(coef(prf_glm))[29:31] # seasons
#length(coef(prf_glm))
loadPkg("aod")  # Analysis of Overdispersed Data, used wald.test in logit example
wald.test(b = coef(prf_glm), Sigma = vcov(prf_glm), Terms = 7:23)
wald.test(b = coef(prf_glm), Sigma = vcov(prf_glm), Terms = 24:28)
wald.test(b = coef(prf_glm), Sigma = vcov(prf_glm), Terms = 29:31)
#drop revenue
res.bestglm <- bestglm(Xy = train3, family = binomial,
IC = "AIC",                 # Information criteria for
method = "exhaustive")
#summary(res.bestglm)
res.bestglm$BestModels
#summary(res.bestglm$BestModels)
prf_glm0 <- glm(y ~  budget + score + vote + company + season, data = train3, family = "binomial")
summary(prf_glm0)
#AIC(prf_glm0) # we can check AIC and BIC
#BIC(prf_glm0)
exp(coef(prf_glm0))
prof_glm_pred = predict(object = prf_glm0, test3, type = c("response")) # predict the model on testing data and return predicted probabilities
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
# best model
prf_Hos0 = hoslem.test(test3$y, prof_glm_pred) # Hosmer and Lemeshow test, a chi-squared test
prf_Hos0
loadPkg("pROC")
h0 <- roc(test3$y ~ prof_glm_pred) # using the model on testing data and see the ROC curve and AUC
auc(h0) # area-under-curve prefer 0.8 or higher.
plot(h0)
title("ROC curve")
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
prf_mcFadden = pR2(prf_glm0)
prf_mcFadden
#confusion matrix
loadPkg("caret")
logit_accuracy <- c(1:5)
logit_kappa <- c(1:5)
threshold_logit <- c(0.5, 0.6, 0.7, 0.8, 0.9) # set threshold for predicted probabilities
j = 1
for (i in c(0.5, 0.6, 0.7, 0.8, 0.9)) {
conf_matrix <- confusionMatrix(data = as.factor(as.integer(prof_glm_pred>i)), reference = test3$y); # using the model on the testing data
logit_accuracy[j] <- conf_matrix$overall[1]*100;
logit_kappa[j] <- conf_matrix$overall[2]
j = j+1
}
# we can see the results at threshold = 0.9 as an example
confusionMatrix(data = as.factor(as.integer(prof_glm_pred>0.54)), reference = test3$y)
# combine results into a dataframe
logit_prediction <- as.data.frame(cbind(threshold = threshold_logit, accuracy = logit_accuracy, kappa = logit_kappa))
logit_prediction
loadPkg("ISLR")
loadPkg("tree")
loadPkg("rpart")
loadPkg("rpart.plot")
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
# we use the same training and testing data so that we can compare Classification Tree to Logit Regression
prf_dt <- rpart(y~., method = "class", data=train3) #rpart to build our tree
summary(prf_dt) # detailed summary of splits
prf_dt$variable.importance#variable importance
#printcp(prf_dt) # display the results
# plot tree
prp(prf_dt, main = "Classification Trees for Profit Status")
#rpart.plot(prf_dt,main = "Classification Trees for Profit Status")
fancyRpartPlot(prf_dt)
printcp(prf_dt) # display the results
plotcp(prf_dt) # visualize cross-validation results
prf_dt_prune <- prune(prf_dt, cp = prf_dt$cptable[which.min(prf_dt$cptable[,"xerror"]),"CP"]) # prune trees at cp where xerror is minimum
#prf_dt_prune <- prune(prf_dt, cp = prf_dt$cptable[4,"CP"])
# plot tree
prp(prf_dt_prune, main = "Classification Trees for Profit Status")
#rpart.plot(prf_dt_prune,main = "Classification Trees for Profit Status")
fancyRpartPlot(prf_dt_prune, main = "Classification Trees for Profit Status")
prf_dt.pred <- predict(prf_dt_prune, test3, type = "class" ) # predict the model on the testing data
table_dt <- table(actual = test3$y, predicted = prf_dt.pred) # create a confusion matrix of predicted and actual values
table_dt
# Accuracy Calculation
sum(diag(table_dt))/sum(table_dt) # sum(true positive + false negative)/ total
confusionMatrix(reference = test3$y, data = prf_dt.pred)
#length(prf_dt.pred)
# I see the method to plot ROC for classification tree in youtube and stackoverflow
# There is an option: type = "prob" when predict rpart object, it return predicted probabilities by the classification tree
# I then realize that the values with predicted probabilites > 0.5 are the values predicted as 1 in the above method (chunk Q92)
# I create the another confusion matrix for this part and set threshold = 0.5 and see that the confusion matrices are also similar
# prediction using type = "prob"
prf_dt.pred1 <- predict(prf_dt_prune, test3, type = "prob" )
#prf_dt.conf <- confusionMatrix(data = as.factor(as.integer(prf_dt.pred1[,2]>0.5)), reference = test3$y)
# the accuracy will be the same as the accuracy we calculate when setting type=class, we can see the results to check it
#prf_dt.conf # the accuracy is also 80.5% and the confusion matrix is the same too
#plot ROC and see AUC
h_dt <- roc(test3$y ~ prf_dt.pred1[,2])
auc(h_dt)
plot(h_dt)
chooseK = function(k, train_set, val_set, train_class, val_class){
# Build knn with k neighbors considered.
class_knn = knn(train = train_set,    #<- training set cases
test = val_set,       #<- test set cases
cl = train_class,     #<- category for classification
k = k,                #<- number of neighbors considered
use.all = TRUE)       #<- control ties between class assignments
#   If true, all distances equal to the kth
#   largest are included
tab = table(class_knn, val_class)
# Calculate the accuracy.
accu = sum(tab[row(tab) == col(tab)]) / sum(tab)
cbind(k = k, accuracy = accu)
}
#test3
#train3
loadPkg("gmodels")
knn_profit <- knn(train = train3[c(1:5)], test = test3[c(1:5)], cl=train3$y, k=3)
CrossTable( knn_profit,test3$y, prop.chisq = FALSE)
chooseK = function(k, train_set, val_set, train_class, val_class){
# Build knn with k neighbors considered.
class_knn = knn(train = train_set,    #<- training set cases
test = val_set,       #<- test set cases
cl = train_class,     #<- category for classification
k = k,                #<- number of neighbors considered
use.all = FALSE)       #<- control ties between class assignments
#   If true, all distances equal to the kth
#   largest are included
tab = table(class_knn, val_class)
# Calculate the accuracy.
accu = sum(tab[row(tab) == col(tab)]) / sum(tab)
cbind(k = k, accuracy = accu)
}
#test3
#train3
loadPkg("gmodels")
knn_profit <- knn(train = train3[c(1:5)], test = test3[c(1:5)], cl=train3$y, k=3)
CrossTable( knn_profit,test3$y, prop.chisq = FALSE)
knn_k_profit = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = train3[c(1:5)],
val_set = test3[c(1:5)],
train_class = train3$y,
val_class = test3$y))
#test3
#train3
loadPkg("gmodels")
knn_profit <- knn(train = train3[c(1:5)], test = test3[c(1:5)], cl=train3$y, k=3)
CrossTable( knn_profit,test3$y, prop.chisq = FALSE)
knn_k_profit = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = train3[c(1:5)],
val_set = test3[c(1:5)],
train_class = train3$y,
val_class = test3$y))
chooseK = function(k, train_set, val_set, train_class, val_class){
# Build knn with k neighbors considered.
class_knn = knn(train = train_set,    #<- training set cases
test = val_set,       #<- test set cases
cl = train_class,     #<- category for classification
k = k,                #<- number of neighbors considered
use.all = TRUE)       #<- control ties between class assignments
#   If true, all distances equal to the kth
#   largest are included
tab = table(class_knn, val_class)
# Calculate the accuracy.
accu = sum(tab[row(tab) == col(tab)]) / sum(tab)
cbind(k = k, accuracy = accu)
}
knn_k_profit = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = train3[c(1:5)],
val_set = test3[c(1:5)],
train_class = train3$y,
val_class = test3$y))
knn_k_profit = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = train3[c(1:5)],
val_set = test3[c(1:5)],
train_class = train3$y,
val_class = test3$y))
knn_k_profit = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = train3[c(1:5)],
val_set = test3[c(1:5)],
train_class = train3$y,
val_class = test3$y))
chooseK = function(k, train_set, val_set, train_class, val_class){
# Build knn with k neighbors considered.
class_knn = knn(train = train_set,    #<- training set cases
test = val_set,       #<- test set cases
cl = train_class,     #<- category for classification
k = k                #<- number of neighbors considered
)       #<- control ties between class assignments
#   If true, all distances equal to the kth
#   largest are included
tab = table(class_knn, val_class)
# Calculate the accuracy.
accu = sum(tab[row(tab) == col(tab)]) / sum(tab)
cbind(k = k, accuracy = accu)
}
#test3
#train3
loadPkg("gmodels")
knn_profit <- knn(train = train3[c(1:5)], test = test3[c(1:5)], cl=train3$y, k=3)
CrossTable( knn_profit,test3$y, prop.chisq = FALSE)
knn_profit <- knn(train = train3[c(1:5)], test = test3[c(1:5)], cl=train3$y, k=7)
CrossTable( knn_profit,test3$y, prop.chisq = FALSE)
confusionMatrix(data=test3$y, reference = knn_profit)
chooseK = function(k, train_set, val_set, train_class, val_class){
# Build knn with k neighbors considered.
class_knn = knn(train = train_set,    #<- training set cases
test = val_set,       #<- test set cases
cl = train_class,     #<- category for classification
k = k                #<- number of neighbors considered
)       #<- control ties between class assignments
#   If true, all distances equal to the kth
#   largest are included
tab = table(class_knn, val_class)
# Calculate the accuracy.
accu = sum(tab[row(tab) == col(tab)]) / sum(tab)
cbind(k = k, accuracy = accu)
}
#test3
#train3
loadPkg("gmodels")
knn_profit <- knn(train = train3[c(1:5)], test = test3[c(1:5)], cl=train3$y, k=3)
CrossTable( knn_profit,test3$y, prop.chisq = FALSE)
knn_k_profit = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = train3[c(1:5)],
val_set = test3[c(1:5)],
train_class = train3$y,
val_class = test3$y))
# Reformat the results to graph the results.
#str(knn_different_k)
knn_k_profit = data.frame(k = knn_k_profit[1,],
accuracy = knn_k_profit[2,])
# Plot accuracy vs. k.
ggplot(knn_k_profit,
aes(x = k, y = accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3) +
theme_bw()
#theme(plot.title = element_text(hjust=0.5, size = 15, face = "bold.italic"))
knn_profit <- knn(train = train3[c(1:5)], test = test3[c(1:5)], cl=train3$y, k=7)
CrossTable( knn_profit,test3$y, prop.chisq = FALSE)
confusionMatrix(data=test3$y, reference = knn_profit)
